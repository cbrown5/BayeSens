% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/kldiv.R
\name{kldiv}
\alias{kldiv}
\title{Estimate the Kullback-Leibler divergence between two random variates}
\usage{
kldiv(
  x1,
  x2,
  nbreaks = 100,
  minx = min(c(x1, x2)),
  maxx = max(c(x1, x2)),
  small = 0.01
)
}
\arguments{
\item{x1}{A \code{numeric} random variate of draws from the posterior
distribution}

\item{x2}{A \code{numeric} random variate of draws from the prior
distribution}

\item{nbreaks}{A single \code{numeric} giving how many breaks to break the
discrete distribution into}

\item{minx}{A single \code{numeric} giving the lower bound of integration}

\item{maxx}{A single \code{numeric} giving the upper bound of integration}
}
\value{
A helldist object containing approximate Hellinger distances and
fitted density kernals.
\item{hdist_disc}{Estimate of Hellinger distance using discrete approximation
of the distributions}
\item{hdist_cont}{Estimate of Hellinger distance using continous
approximation
of distributions}
}
\description{
Estimate the Kullback-Leibler divergence between two random variates
}
\details{
Kullback-Leibler divergence is approximated by
binning the random variates and calculating the KL-div
for discrete distributions.

It is recommended to visually
check distribution fits, particularly if the number of random variates is
small.

In general this methods will be inaccurate if analysis is performed on
too few samples, e.g. <10 000. >100 000 would be ideal.
}
\author{
Christopher J. Brown christo.j.brown@gmail.com
}
